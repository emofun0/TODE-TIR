      
# Fine-tuning configuration for TIR-aware TODE model
# Based on train_tir.yaml but optimized for transfer learning

"model":
  "type": "todetir"
  "params":
    "lambda_val": 1
    "res": True

"optimizer":
  "type": "AdamW"
  "params":
    "lr": 0.00005  # Much lower learning rate for fine-tuning (5e-5)
    "weight_decay": 0.01  # Add weight decay for regularization

"lr_scheduler":
  "type": "MultiStepLR"
  "params":
    "milestones": [10, 15, 18]  # Earlier milestones for shorter fine-tuning
    "gamma": 0.5  # More conservative learning rate decay

"dataset":
  "train":
    "type": "transpose"
    "data_dir": "/home/data/qiuguanhe/TRansPose"
    "image_size": !!python/tuple [320, 240]
    "use_augmentation": True
    "rgb_augmentation_probability": 0.6  # Reduced augmentation for fine-tuning
    "use_depth_augmentation": True
    "depth_min": 0.0
    "depth_max": 10.0
    "depth_norm": 1.0
  "test":
    "type": "transpose"
    "data_dir": "/home/data/qiuguanhe/TRansPose"
    "image_size": !!python/tuple [320, 240]
    "use_augmentation": False
    "depth_min": 0.0
    "depth_max": 10.0
    "depth_norm": 1.0

"dataloader":
  "num_workers": 48
  "shuffle": True
  "drop_last": True

"trainer":
  "batch_size": 16  # Slightly larger batch size for stable gradients
  "test_batch_size": 16
  "multigpu": False
  "max_epoch": 240  # Shorter training for fine-tuning
  "criterion":
    "type": "custom_masked_l1_loss"
    "epsilon": 0.00000001
    "combined_smooth": True
    "combined_beta": 0.1

"metrics":
  "types": ["MaskedMSE", "MaskedRMSE", "MaskedREL", "MaskedMAE", "MaskedThreshold@1.05", "MaskedThreshold@1.10", "MaskedThreshold@1.25"]
  "epsilon": 0.00000001
  "depth_scale": 1.0

"stats":
  "stats_dir": "stats"
  "stats_exper": "finetune-tir"  # Different experiment name

"tb_log":
  "stats_dir": "tb_log"
  "stats_exper": "finetune-tir"  # Different experiment name

    